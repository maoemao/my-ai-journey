# LangChainæœåŠ¡ç«¯éƒ¨ç½²ä»¥åŠé“¾è·¯ç›‘æ§

éå¸¸å¥½ï¼Œè¿™ä¸ªé—®é¢˜é—®åˆ° LangChain çš„**ç”Ÿäº§è½åœ°å…³é”®ç‚¹**â€”â€”ä½ ä»ã€Œæ¨¡å‹è°ƒç”¨ã€è¿›é˜¶åˆ°ã€Œç³»ç»Ÿçº§åº”ç”¨ã€çš„é˜¶æ®µäº†ã€‚

æˆ‘ä»¬æ¥ç³»ç»Ÿè®²è§£ä¸€ä¸‹ï¼š

> âœ… ç›®æ ‡ï¼šè®© LangChain é¡¹ç›®åœ¨æœåŠ¡ç«¯å¯ç¨³å®šéƒ¨ç½²ã€å¯è§‚æµ‹ã€å¯æ‰©å±•ã€‚



------



## **ğŸ§© ä¸€ã€æ•´ä½“éƒ¨ç½²æ€è·¯**



LangChain æœ¬èº«æ˜¯ä¸€ä¸ª**æ¡†æ¶ï¼Œä¸æ˜¯æœåŠ¡**ï¼Œæ‰€ä»¥éƒ¨ç½²æ—¶æˆ‘ä»¬é€šå¸¸éœ€è¦ï¼š



1. æŠŠ LangChain åº”ç”¨å°è£…ä¸º API æœåŠ¡ï¼›
2. éƒ¨ç½²åˆ°æœåŠ¡å™¨ï¼ˆDockerã€K8s æˆ–äº‘å‡½æ•°ï¼‰ï¼›
3. é…åˆæ—¥å¿—ã€ç›‘æ§ã€Tracing ç³»ç»Ÿå®ç°é“¾è·¯è§‚æµ‹ã€‚

------



## **ğŸ§± äºŒã€æœåŠ¡ç«¯éƒ¨ç½²çš„å¸¸è§æ–¹å¼**



### **1ï¸âƒ£ FastAPI éƒ¨ç½²ï¼ˆæœ€å¸¸ç”¨ï¼‰**



LangChain å®˜æ–¹æ¨èçš„æ–¹å¼å°±æ˜¯åŸºäº **FastAPI** æˆ– **Flask** å°è£…æˆ HTTP æ¥å£ã€‚

#### **ç¤ºä¾‹ï¼š**



```
from fastapi import FastAPI, Request
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser

app = FastAPI()

llm = ChatOpenAI(model="gpt-4o")
prompt = ChatPromptTemplate.from_template("ä½ æ˜¯ä¸€ååŠ©æ‰‹ï¼Œå›ç­”ï¼š{question}")
chain = prompt | llm | StrOutputParser()

@app.post("/chat")
async def chat(req: Request):
    data = await req.json()
    question = data.get("question")
    result = await chain.ainvoke({"question": question})
    return {"answer": result}
```

ç„¶åï¼š

```
uvicorn main:app --host 0.0.0.0 --port 8000
```

ğŸ’¡ **ç”Ÿäº§å»ºè®®ï¼š**

- ä½¿ç”¨ gunicorn + uvicorn.workers.UvicornWorker ä½œä¸ºç”Ÿäº§å…¥å£ï¼›
- ä½¿ç”¨ Docker å°è£…éƒ¨ç½²ã€‚

------



### **2ï¸âƒ£ Docker å®¹å™¨åŒ–**



#### **Dockerfile ç¤ºä¾‹ï¼š**



```
FROM python:3.11-slim
WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000
CMD ["gunicorn", "-k", "uvicorn.workers.UvicornWorker", "main:app", "--bind", "0.0.0.0:8000"]
```



#### **å¯åŠ¨ï¼š**



```
docker build -t langchain-service .
docker run -d -p 8000:8000 langchain-service
```



------



### **3ï¸âƒ£ LangServeï¼šLangChain å®˜æ–¹éƒ¨ç½²æ¡†æ¶**



> âœ… **æ¨èï¼**

> LangServe æ˜¯ LangChain å®˜æ–¹æ¨å‡ºçš„æœåŠ¡åŒ–å±‚ï¼Œç”¨äºç›´æ¥æŠŠ LCEL é“¾è·¯å‘å¸ƒä¸º APIã€‚

```
# app.py
from langserve import add_routes
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from fastapi import FastAPI

llm = ChatOpenAI(model="gpt-4o")
prompt = ChatPromptTemplate.from_template("å›ç­”ï¼š{question}")
chain = prompt | llm | StrOutputParser()

app = FastAPI()
add_routes(app, chain, path="/qa")  # è‡ªåŠ¨æš´éœ²API

# è¿è¡Œ
# uvicorn app:app --reload
```

è®¿é—® /docs å°±èƒ½çœ‹åˆ°è‡ªåŠ¨ç”Ÿæˆçš„ Swagger APIã€‚

**ä¼˜åŠ¿ï¼š**

- è‡ªåŠ¨ JSON åŒ–è¾“å…¥è¾“å‡ºï¼›
- å†…ç½®å¼‚æ­¥æµå¼è¾“å‡ºï¼›
- è‡ªåŠ¨ç”Ÿæˆ OpenAPI æ–‡æ¡£ï¼›
- ä¸ LangSmith æ— ç¼é›†æˆé“¾è·¯ç›‘æ§ã€‚



------



## **ğŸ“Š ä¸‰ã€é“¾è·¯ç›‘æ§ä¸è§‚æµ‹ä½“ç³»**



LangChain æä¾›äº†å®˜æ–¹å¯è§†åŒ–ç›‘æ§å·¥å…·ï¼š



### **ğŸ”¹ LangSmithï¼ˆå®˜æ–¹è§‚æµ‹å¹³å°ï¼‰**

> LangSmith = LangChain çš„ APM + è°ƒè¯•å™¨

> å¯ä»¥çœ‹åˆ°æ¯æ¬¡ LLM è°ƒç”¨çš„è¾“å…¥ã€è¾“å‡ºã€æ‰§è¡Œè€—æ—¶ã€åµŒå¥—è°ƒç”¨å…³ç³»ã€‚



------



### **âœ³ï¸ é›†æˆæ–¹æ³•ï¼š**



1ï¸âƒ£ å®‰è£…ï¼š

```
pip install langsmith
```

2ï¸âƒ£ é…ç½®ç¯å¢ƒå˜é‡ï¼š

```
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY="ä½ çš„LangSmith API Key"
export LANGCHAIN_PROJECT="my-langchain-app"
```

3ï¸âƒ£ åœ¨ä»£ç ä¸­è‡ªåŠ¨ç”Ÿæ•ˆï¼š

ä»»ä½• chain.invoke()ã€agent.run() çš„æ‰§è¡Œéƒ½ä¼šè¢«è¿½è¸ªã€‚

4ï¸âƒ£ è®¿é—®ï¼š

ç™»å½• https://smith.langchain.com![Attachment.tiff](Attachment.tiff) æŸ¥çœ‹è°ƒç”¨æ—¥å¿—ä¸é“¾è·¯ã€‚



------



### **ğŸ“¡ å¯è§‚æµ‹ä¿¡æ¯åŒ…æ‹¬ï¼š**



| **æŒ‡æ ‡**    | **è¯´æ˜**                |
| ----------- | ----------------------- |
| è¾“å…¥ / è¾“å‡º | æ¯ä¸ªé“¾çš„ prompt ä¸ç»“æœ  |
| è°ƒç”¨è€—æ—¶    | å„æ­¥éª¤æ‰§è¡Œæ—¶é—´          |
| å·¥å…·è°ƒç”¨æ ˆ  | å“ªä¸ªAgentè°ƒç”¨äº†å“ªä¸ªå·¥å…· |
| Token æ¶ˆè€—  | æˆæœ¬åˆ†æ                |
| é”™è¯¯æ ˆ      | æŠ¥é”™ä¸å¤±è´¥é“¾è·¯åˆ†æ      |



------



## **âš™ï¸ å››ã€è¿›é˜¶ï¼šè‡ªå»ºç›‘æ§ + æ—¥å¿—ä½“ç³»**



å¦‚æœä¸æƒ³ç”¨ LangSmithï¼ˆå› ä¸ºç§æœ‰éƒ¨ç½²æˆ–å†…ç½‘ç¯å¢ƒï¼‰ï¼Œå¯ä»¥ï¼š

- ä½¿ç”¨ **CallbackManager** è‡ªå®šä¹‰ç›‘æ§ï¼›
- æŠŠæ¯æ¬¡æ‰§è¡Œä¿¡æ¯è¾“å‡ºåˆ°æ—¥å¿—æˆ– Prometheusã€‚



### **ç¤ºä¾‹ï¼š**



```
from langchain.callbacks.base import BaseCallbackHandler

class MyLogger(BaseCallbackHandler):
    def on_llm_start(self, serialized, prompts, **kwargs):
        print(f"LLMå¼€å§‹: {prompts}")

    def on_llm_end(self, response, **kwargs):
        print(f"LLMç»“æŸ: {response.generations[0][0].text}")

from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model="gpt-4o", callbacks=[MyLogger()])
llm.invoke("ä½ å¥½")
```

> ä½ å¯ä»¥æ‰©å±• MyLoggerï¼ŒæŠŠç›‘æ§æ•°æ®æ¨åˆ° Prometheusã€Elasticsearch æˆ– Grafana Dashboardã€‚



------



## **ğŸ§  äº”ã€éƒ¨ç½²æ¶æ„æ¨èï¼ˆç”Ÿäº§çº§ï¼‰**



| **å±‚çº§**   | **æŠ€æœ¯é€‰æ‹©**                     | **è¯´æ˜**              |
| ---------- | -------------------------------- | --------------------- |
| **Web å±‚** | FastAPI / LangServe              | æä¾› API æ¥å£         |
| **é€»è¾‘å±‚** | LangChain LCEL / Agent           | æ‰§è¡Œä¸šåŠ¡é€»è¾‘          |
| **æ¨¡å‹å±‚** | OpenAI / Ollama / LocalLLM       | æ¨¡å‹è°ƒç”¨å±‚            |
| **ç›‘æ§å±‚** | LangSmith / Prometheus + Grafana | é“¾è·¯è§‚æµ‹              |
| **æ—¥å¿—å±‚** | ELK / Loki                       | æ—¥å¿—è¿½è¸ª              |
| **ç¼“å­˜å±‚** | Redis / LangCache                | æé€Ÿä¸èŠ‚çœ Token æˆæœ¬ |



------



## **ğŸš€ å…­ã€å®æˆ˜å»ºè®®**



| **ç›®æ ‡**          | **æ¨èæ–¹æ¡ˆ**                           |
| ----------------- | -------------------------------------- |
| å¿«é€Ÿä¸Šçº¿ MVP      | FastAPI + LangServe                    |
| ä¼ä¸šå†…ç½‘ç§æœ‰éƒ¨ç½²  | Docker + LangServe + Callbackç›‘æ§      |
| å¤§è§„æ¨¡è§‚æµ‹ / ä¼˜åŒ– | LangSmith + Grafana                    |
| æˆæœ¬æ§åˆ¶          | å¯ç”¨ç¼“å­˜ (LangChain Cache / Redis)     |
| è‡ªåŠ¨é‡è¯•ä¸å›é€€    | LCEL .with_retry() + .with_fallbacks() |



------

