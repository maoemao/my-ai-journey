# LlamaIndexå¤§æ¨¡å‹åº”ç”¨å¼€å‘å¿«é€Ÿå…¥é—¨

ä¸‹é¢æˆ‘ç»™ä½ ä¸€ä»½ **å®æˆ˜æ´¾ã€ç¨‹åºå‘˜è§†è§’çš„ã€ŠLlamaIndex å¤§æ¨¡å‹åº”ç”¨å¼€å‘å¿«é€Ÿå…¥é—¨ã€‹**ï¼Œçœ‹å®Œä½ èƒ½ç«‹åˆ»ç”¨å®ƒæ­å»ºä¸€ä¸ªå¯ç”¨çš„ LLM åº”ç”¨ï¼ˆRAGã€Agentã€å·¥å…·è°ƒç”¨éƒ½èƒ½åšï¼‰ã€‚

å†…å®¹å°½é‡ â€œç›´æ¥ç›´å»â€ï¼Œé¿å…åºŸè¯ â€”â€” ä½ çœ‹å®Œå°±èƒ½å¼€å¹²ã€‚

------

# ğŸš€ ä¸€ã€LlamaIndex æ˜¯ä»€ä¹ˆï¼Ÿ

ä¸€å¥è¯ï¼š

> **LlamaIndex = å¤§æ¨¡å‹åº”ç”¨å¼€å‘æ¡†æ¶ï¼Œç”¨æ¥å¿«é€Ÿæ„å»º RAGã€Agentã€å·¥å…·è°ƒç”¨ã€æ•°æ®å¤„ç†æµæ°´çº¿ã€‚**

å®ƒä¸»è¦è§£å†³ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š

1. **å¤–éƒ¨æ•°æ®å¦‚ä½•æ¥å…¥å¤§æ¨¡å‹ï¼ˆæ•°æ®è¿æ¥ï¼‰**
2. **å¦‚ä½•æŠŠæ–‡æœ¬å˜æˆå‘é‡ã€ç´¢å¼•ã€æ£€ç´¢ç»“æ„ï¼ˆç´¢å¼•ï¼‰**
3. **å¦‚ä½•è®©æ¨¡å‹â€œæŸ¥èµ„æ–™ + æ€è€ƒ + å›ç­”â€ï¼ˆæŸ¥è¯¢/Agentï¼‰**

é€šä¿—ç†è§£ï¼š
 å®ƒæ¯” LangChain æ›´åå‘ **æ•°æ®ä¸ºä¸­å¿ƒ**ï¼ŒRAG èƒ½åŠ›éå¸¸å¼ºã€‚

------

# ğŸš§ äºŒã€æœ€å¸¸ç”¨çš„åŠŸèƒ½ï¼ˆç®€å•è®°ä½è¿™ 6 ä¸ªï¼‰

LlamaIndex åŠŸèƒ½å¾ˆå¤šï¼Œä½†ä½œä¸ºå¼€å‘è€…ï¼Œä½ æœ€å¸¸ç”¨çš„æ˜¯ï¼š

1. **Readersï¼ˆæ–‡æ¡£åŠ è½½å™¨ï¼‰**
   - PDFã€Markdownã€ç½‘é¡µã€æ•°æ®åº“
2. **Node Parserï¼ˆæŠŠæ–‡æ¡£åˆ‡æˆ chunkï¼‰**
3. **Storage Contextï¼ˆå­˜å‚¨ï¼‰**
4. **Vector Storeï¼ˆå‘é‡æ•°æ®åº“æ¥å£ï¼‰**
   - æ”¯æŒ Milvusã€Pineconeã€Chromaã€Elasticsearchâ€¦
5. **Indexï¼ˆç´¢å¼•ç»“æ„ï¼‰**
   - VectorIndex
   - TreeIndex
   - GraphIndex
   - KeywordTableIndex
6. **Query Engineï¼ˆæŸ¥è¯¢å¼•æ“ï¼‰**
   - RAG ä¸»é€»è¾‘
   - å¯æ¥å…¥å·¥å…·ã€Agent

------

# ğŸ›  ä¸‰ã€5 åˆ†é’Ÿè·‘é€šä¸€ä¸ª RAG Demoï¼ˆä½ å¯ä»¥å¤åˆ¶ç›´æ¥ç”¨ï¼‰

## 1. å®‰è£…

```bash
pip install llama-index
pip install llama-index-embeddings-openai
pip install llama-index-llms-openai
```

## 2. é…ç½®å¯†é’¥

```python
import os
os.environ["OPENAI_API_KEY"] = "ä½ çš„key"
```

ï¼ˆä¹Ÿå¯ä»¥æ¢æˆ DeepSeekã€Qwenï¼Œæœ¬è´¨ä¸€æ ·ï¼‰

## 3. åŠ è½½æ–‡æ¡£

```python
from llama_index.core import SimpleDirectoryReader

documents = SimpleDirectoryReader("docs").load_data()
```

## 4. æ„å»ºç´¢å¼•ï¼ˆRAG æ ¸å¿ƒï¼‰

```python
from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_documents(documents)
```

## 5. åˆ›å»ºæŸ¥è¯¢å¼•æ“

```python
query_engine = index.as_query_engine()
```

## 6. å¼€å§‹é—®é—®é¢˜

```python
response = query_engine.query("æ–‡æ¡£é‡Œæåˆ°çš„äº§å“ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ")
print(response)
```

**å°±æ˜¯è¿™ä¹ˆç®€å•ã€‚**

------

# ğŸ” å››ã€æ ¸å¿ƒåŸç†ï¼ˆRAG çš„ 3 ä¸ªè¿‡ç¨‹ï¼‰

### 1ï¼‰å°†æ–‡æ¡£è§£ææˆ Node

ï¼ˆç­‰ä»·äº chunkï¼‰

### 2ï¼‰å°† Node ç¼–ç æˆ embeddingï¼Œå†™å…¥å‘é‡åº“

### 3ï¼‰ç”¨æˆ·æé—® â†’ æ£€ç´¢ â†’ ç»„è£… Prompt â†’ LLM å›ç­”

è¿™ 3 æ­¥å…¨éƒ¨ç”± LlamaIndex è‡ªåŠ¨åšå®Œã€‚

------

# ğŸ§± äº”ã€æ¨èçš„é¡¹ç›®ç»“æ„ï¼ˆä½ çš„é¡¹ç›®å»ºè®®è¿™ä¹ˆå†™ï¼‰

```
my-rag-app/
â”‚â”€â”€ data/                # PDFã€md æ–‡ä»¶
â”‚â”€â”€ config.yaml          # LLMã€embedding é…ç½®
â”‚â”€â”€ build_index.py       # æ„å»ºç´¢å¼•è„šæœ¬
â”‚â”€â”€ serve.py             # æä¾› RAG æ¥å£
â”‚â”€â”€ llm/                 # llm é…ç½®ç›®å½•
â”‚â”€â”€ vector/              # å‘é‡åº“
```

------

# ğŸ”¥ å…­ã€æŠŠ LlamaIndex æ¢æˆå›½äº§æ¨¡å‹ï¼ˆQwen / DeepSeekï¼‰

ç¤ºä¾‹ï¼ˆDeepSeek ä½œä¸º LLMï¼‰ï¼š

```bash
pip install llama-index-llms-openai
```

DeepSeek å…¼å®¹ OpenAI APIï¼š

```python
from llama_index.llms.openai import OpenAI

llm = OpenAI(
    api_base="https://api.deepseek.com/v1",
    api_key="ä½ çš„key",
    model="deepseek-chat"
)
```

Embedding ç”¨ OpenAI å…¼å®¹æ¥å£å³å¯ï¼š

```python
from llama_index.embeddings.openai import OpenAIEmbedding

embed = OpenAIEmbedding(
    api_base="https://api.deepseek.com/v1",
    api_key="ä½ çš„key",
    model="deepseek-embedding"
)
```

ç„¶åï¼š

```python
query_engine = index.as_query_engine(llm=llm)
```

------

# ğŸ§© ä¸ƒã€LlamaIndex vs LangChainï¼ˆä½ ä¸€å®šä¼šé—®ï¼‰

| åŠŸèƒ½       | LlamaIndex | LangChain |
| ---------- | ---------- | --------- |
| RAG å¼ºåº¦   | â­â­â­â­â­      | â­â­â­       |
| å¯è§£é‡Šæ€§   | â­â­â­â­â­      | â­â­        |
| Agent      | â­â­â­        | â­â­â­â­â­     |
| ç¤¾åŒºç”Ÿæ€   | â­â­â­        | â­â­â­â­â­     |
| æ•°æ®ä¸ºä¸­å¿ƒ | å¼º         | ä¸€èˆ¬      |
| å·¥å…·é›†æˆ   | ä¸€èˆ¬       | å¼º        |

æ€»ç»“ï¼š

- **åš RAG â†’ ç”¨ LlamaIndex æ›´ç®€å•æ›´å¼º**
- **åš Agentã€å·¥å…·è°ƒç”¨ â†’ LangChain æ›´å¥½**
- **æ··åˆç”¨ï¼ˆæœ€å¸¸è§ï¼‰â†’ LlamaIndex åŠ è½½æ•°æ® + LangChain Agent**

------

# ğŸš€ å…«ã€å¿«é€Ÿæ„å»ºä¸€ä¸ªä¼ä¸šçº§ RAGï¼ˆä½ å®é™…å¯èƒ½éœ€è¦ï¼‰

ä¸‹é¢æ˜¯å…¸å‹ä¼ä¸šéœ€æ±‚ï¼š

- æ–‡æ¡£ä¸Šä¼ 
- è‡ªåŠ¨åˆ‡ç‰‡
- è‡ªåŠ¨ç´¢å¼•
- å‘é‡åº“ï¼ˆMilvus/PGVectorï¼‰
- LLMï¼ˆDeepSeek/Qwenï¼‰
- Spark/æ··åˆæ£€ç´¢
- å·¥ç¨‹åŒ–éƒ¨ç½²ï¼ˆFastAPIï¼‰

LlamaIndex å·²æœ‰å®Œæ•´æ”¯æŒï¼š

```python
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
```

æˆ–æ­å»ºæ··åˆæ£€ç´¢ï¼š

```python
from llama_index.core.retrievers import (
    VectorIndexRetriever,
    KeywordTableSimpleRetriever
)
from llama_index.core.query_engine import RouterQueryEngine
```

------

# ğŸ”‹ ä¹ã€å®ç”¨æŠ€å·§ï¼ˆä½ ç°åœ¨å°±èƒ½ç”¨ï¼‰

### âœ” 1. ä¸è¦ç”¨é»˜è®¤ embeddings

ä¸­æ–‡ä»»åŠ¡å¼ºçƒˆæ¨èï¼š

- **text-embedding-3-large**ï¼ˆOpenAIï¼‰
- **DeepSeek embedding**
- **Qwen Embedding**

### âœ” 2. æ–‡æ¡£å¿…é¡»åˆ‡ç‰‡

é»˜è®¤ chunk_size = 512 å¾ˆåˆç†ã€‚

### âœ” 3. ç¡®ä¿ç´¢å¼•æ”¾åœ¨æœ¬åœ°æˆ– Redisï¼Œä¸è¦æ”¾ RAM

å¦åˆ™æœåŠ¡é‡å¯ä¼šä¸¢å¤±ã€‚

### âœ” 4. é¿å…â€œé•¿æ–‡å…¨æŠ•å–‚å¤§æ¨¡å‹â€

æ£€ç´¢åˆ°çš„ chunks æœ€å¤š 5~10 ä¸ªã€‚

------

# ğŸ§  åã€ä½œä¸ºå¯¼å¸ˆï¼Œæˆ‘ç»™ä½ çš„å…¥é—¨è·¯çº¿ï¼ˆéå¸¸å®ç”¨ï¼‰

1. **10 åˆ†é’Ÿè·‘é€šåŸºæœ¬ RAG Demoï¼ˆä½ å·²ç»çœ‹åˆ°äº†ï¼‰**
2. **æŠŠç´¢å¼•å­˜å…¥å‘é‡æ•°æ®åº“ï¼ˆMilvus/Chromaï¼‰**
3. **æ¥å…¥ DeepSeek API åšå›ç­”**
4. **ç”¨ FastAPI å°è£…æˆæ¥å£**
5. **åŠ å…¥ Hybrid Retrievalï¼ˆBM25 + vectorï¼‰**
6. **ç”¨ JSON mode + ReAct åšå·¥å…·å¢å¼º**
7. **æœ€åç»“åˆ LangChain åš Agent**

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¤§æ¨¡å‹å¼€å‘å…¥é—¨è·¯çº¿ã€‚

------

