# å¦‚ä½•é’ˆå¯¹è¶…é•¿æ–‡æœ¬è¿›è¡Œå¾®è°ƒè®­ç»ƒ

ä¸‹é¢æˆ‘ç»™ä½  **å®è¯å®è¯´ã€å¯è½åœ°çš„æ–¹æ¡ˆ**ï¼šé’ˆå¯¹ **è¶…é•¿æ–‡æœ¬ï¼ˆ>512 tokensï¼‰** è¦å¾®è°ƒè®­ç»ƒï¼ŒBERT è¿™ç§ç¡¬é™åˆ¶æ¨¡å‹æœ¬èº«åšä¸äº†ï¼Œä½ å¿…é¡»æ¢ç­–ç•¥ã€‚ä¸‹é¢æˆ‘ç›´æ¥æŠŠ **å¯è¡Œçš„ä¸‰å¥—è®­ç»ƒè·¯çº¿** éƒ½è®²æ¸…æ¥šï¼Œå¹¶ç»™å‡ºä»£ç å®ç°æ–¹å¼ã€‚

------

# ğŸŒŸ æ€»ç»“ä¸€å¥è¯

**é•¿æ–‡æœ¬ä»»åŠ¡è¦ä¹ˆæ¢é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ï¼ˆLongformer/RoBERTa-long/BigBirdï¼‰ï¼Œè¦ä¹ˆåˆ‡ç‰‡ + èšåˆå¤„ç†ï¼Œæˆ–è€…ç”¨ LongLoRA ç­‰æŠ€æœ¯æ‰©å±•ä¸Šä¸‹æ–‡ã€‚BERT åŸç”Ÿ 512 æ˜¯æ­»é™åˆ¶ã€‚**

------

# æ–¹æ¡ˆ Aï¼šä½¿ç”¨æ”¯æŒé•¿ä¸Šä¸‹æ–‡çš„ Transformerï¼ˆå¼ºçƒˆæ¨èï¼‰

å¦‚æœä½ çš„ä»»åŠ¡çœŸçš„éœ€è¦åƒå®Œæ•´é•¿æ–‡æœ¬ï¼Œæ¯”å¦‚ï¼š

- æ³•å¾‹é•¿æ–‡åˆ†ç±»
- å­¦æœ¯æ–‡ç« åˆ†ç±»
- æŠ€æœ¯æ–‡æ¡£åˆ†ç±»
- å¤šåƒå­—æƒ…æ„Ÿåˆ†æ
- äº‹ä»¶è¯†åˆ«

**ç›´æ¥æ¢æ¨¡å‹æ˜¯æœ€å¹²å‡€çš„è·¯çº¿ã€‚**

## å¯ç”¨çš„é•¿æ–‡æœ¬æ¨¡å‹

| æ¨¡å‹                         | æ”¯æŒé•¿åº¦               | ç‰¹ç‚¹               |
| ---------------------------- | ---------------------- | ------------------ |
| **Longformer**               | ~4K                    | æœ€ç»å…¸             |
| **BigBird-roberta**          | ~4K                    | Googleï¼Œæ€§èƒ½å¼º     |
| **RoBERTa-large-long**       | 4K~16K                 | HuggingFace ç¤¾åŒºç‰ˆ |
| **BERT-extended (512â†’4096)** | è‡ªå·±æ”¹ Rotary/ä½ç½®ç¼–ç  | ç¨å¤æ‚             |

### Longformer å¾®è°ƒç¤ºä¾‹

```python
from transformers import LongformerForSequenceClassification, LongformerTokenizer

model = LongformerForSequenceClassification.from_pretrained(
    "allenai/longformer-base-4096", 
    num_labels=2
)
tokenizer = LongformerTokenizer.from_pretrained("allenai/longformer-base-4096")

def encode(batch):
    return tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=4096
    )
```

å‰©ä¸‹æµç¨‹ä¸ BERT å¾®è°ƒä¸€æ ·ã€‚

âœ” è¿™æ˜¯æœ€çœå¿ƒå’Œæ•ˆæœæœ€å¥½çš„è·¯çº¿ã€‚

------

# æ–¹æ¡ˆ Bï¼šä½¿ç”¨ BERTï¼ˆæˆ– RoBERTaï¼‰+ é•¿æ–‡æœ¬åˆ†ç‰‡ï¼ˆæ»‘çª—ï¼‰+ èšåˆï¼ˆå¯æ§ä¸”ç¨³å®šï¼‰

å¦‚æœä½ å¿…é¡» â€œç”¨ BERT å¾®è°ƒâ€ï¼Œé‚£åªèƒ½é‡‡ç”¨ **åˆ‡ç‰‡ï¼ˆchunkingï¼‰ç­–ç•¥**ï¼ŒæŠŠè¶…é•¿æ–‡æœ¬åˆ‡å¼€åå†èšåˆç»“æœã€‚

## å¸¸è§åˆ‡ç‰‡ç­–ç•¥

### ç­–ç•¥ 1ï¼šå›ºå®šçª—å£ + é‡å  sliding windowï¼ˆæœ€ä¸»æµï¼‰

ä¾‹å¦‚ï¼š

```
max_length = 256
stride = 128   â† é‡å 
```

æ¯ä¸ª chunk è¿‡ BERT â†’ å¾—åˆ°ä¸€å † logits â†’ å†åšèšåˆï¼š

- åˆ†ç±»ä»»åŠ¡ â†’ å¹³å‡ã€maxã€åŠ æƒ
- å¤šæ ‡ç­¾ä»»åŠ¡ â†’ é€ç‰‡ OR
- QA â†’ logits æ‹¼æ¥å¯»æ‰¾æœ€ä¼˜ span

### ç¤ºä¾‹ä»£ç ï¼ˆæ–‡æœ¬åˆ†ç±»ï¼‰

```python
def chunk_text(text, tokenizer, max_len=256, stride=128):
    tokens = tokenizer(text, return_tensors="pt", truncation=False)
    input_ids = tokens["input_ids"][0]
    chunks = []

    start = 0
    while start < len(input_ids):
        end = start + max_len
        chunk_ids = input_ids[start:end]
        chunk_ids = tokenizer.build_inputs_with_special_tokens(chunk_ids)
        chunks.append(chunk_ids)
        if end >= len(input_ids): break
        start += max_len - stride
    return chunks
```

è®­ç»ƒæ—¶å¯¹æ¯ä¸ª chunk è®­ç»ƒä¸€ä¸ªæ ‡ç­¾å³å¯ï¼Œæ¨ç†æ—¶è¯»å–æ‰€æœ‰ chunk logitsï¼š

```python
logits_list = []
for chunk in chunks:
    out = model(input_ids=chunk.unsqueeze(0))
    logits_list.append(out.logits)

final_logits = torch.mean(torch.stack(logits_list), dim=0)
```

âœ” ä¼˜ç‚¹ï¼š

- ä¿ç•™ BERT å‚æ•°
- æ˜¾å­˜å°
- ä¸ç”¨ç‰¹åˆ¶æ¨¡å‹

âœ˜ ç¼ºç‚¹ï¼š

- â€œè·¨ chunk è¯­ä¹‰â€ ä¼šæŸå¤±

------

# æ–¹æ¡ˆ Cï¼šæ‰©å±• BERT ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆLongLoRAã€RoPE Scalingã€Position Embedding Interpolationï¼‰

å¦‚æœä½ è¿½æ±‚â€œå°½å¯èƒ½ä¿ç•™ BERT çš„èƒ½åŠ›â€ï¼Œä½†éœ€è¦æ›´é•¿ä¸Šä¸‹æ–‡ï¼Œå¯ä»¥è€ƒè™‘ **æ‰©é•¿ä½ç½®ç¼–ç **ã€‚

### æ–¹æ³•ï¼š

- å¯¹å¯å­¦ä¹  position embedding åšæ’å€¼
- å¼•å…¥ RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰
- ä½¿ç”¨ LongLoRA æŠ€æœ¯å¾®è°ƒ
- ä¿®æ”¹æ¨¡å‹ config çš„ `max_position_embeddings`ï¼Œå†é‡æ–°åˆå§‹åŒ– pos embedding

ä¸è¿‡æˆ‘å®è¯è¯´ï¼š

> **æ”¹ BERT çš„ä½ç½®ç¼–ç å¯¹æ™®é€šäººæ¥è¯´å‘éå¸¸å¤šï¼Œä¸å¦‚ç›´æ¥æ¢ Longformer/BigBirdã€‚**

ä½†ä½ è¦åšï¼Œæˆ‘ä¹Ÿç»™ä½  demoï¼š

### æ’å€¼ position embedding ç¤ºä¾‹

```python
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-chinese")
new_len = 4096

old_pos = model.embeddings.position_embeddings.weight
new_pos = torch.nn.functional.interpolate(
    old_pos.unsqueeze(0).transpose(1,2),
    size=new_len,
    mode='linear',
    align_corners=False
).transpose(1,2).squeeze(0)

model.embeddings.position_embeddings = torch.nn.Embedding.from_pretrained(new_pos)
model.config.max_position_embeddings = new_len
```

ç„¶åå¸¦ç€æ–°çš„ pos embedding ç»§ç»­å¾®è°ƒå³å¯ã€‚

âœ” ä¼˜ç‚¹ï¼šä¸ç”¨æ¢æ¨¡å‹
 âœ˜ å‘å¤šã€æ•ˆæœä¸ç¨³å®šã€æ˜¾å­˜æ›´é«˜

------

# ä½ åº”è¯¥é€‰å“ªæ¡è·¯çº¿ï¼Ÿ

æˆ‘ç»™ä½ å¿«é€Ÿåˆ¤æ–­ï¼š

| åœºæ™¯                          | æ¨èæ–¹æ¡ˆ                   |
| ----------------------------- | -------------------------- |
| æ–‡æœ¬å¿…é¡»æ•´ä½“åƒ                | **Longformer / BigBird**   |
| å¸Œæœ›ç»§ç»­ç”¨ BERTã€æ˜¾å­˜æœ‰é™     | **BERT åˆ‡ç‰‡ + èšåˆ**       |
| æƒ³æ‰©é•¿ BERTï¼Œä½†èƒ½æ‰¿å—è°ƒè¯•æˆæœ¬ | **ä½ç½®ç¼–ç æ’å€¼**           |
| å‡ ä¸‡å­—æ–‡æ¡£ï¼ˆæ³•å¾‹/ä¸“åˆ©ï¼‰       | **BigBird / RoBERTa-long** |

------

