# DeepSeek结合llamaIndex构建RAG应用

下面我给你 **一套真正可落地的《DeepSeek + LlamaIndex 构建 RAG 应用》完整方案**，从架构 → 代码 → 部署一步到位，让你直接能在实际项目里用。

内容全部以 **DeepSeek（OpenAI 格式 API） + LlamaIndex（RAG 框架）** 为核心。

------

# 🚀 一、整体架构（最符合你实际开发需求）

RAG 的完整链路如下：

```
文档 → 解析 → 切片 → embedding → 向量库 → 检索 → Prompt 组装 → DeepSeek 回答
```

对应到 LlamaIndex：

| RAG 阶段 | LlamaIndex 组件  | 说明                            |
| -------- | ---------------- | ------------------------------- |
| 文档读取 | Reader           | PDF / MD / 网页                 |
| 文档切片 | NodeParser       | chunk + metadata                |
| 向量化   | Embedding        | DeepSeek/Other                  |
| 存储     | VectorStore      | Chroma/Milvus/Weaviate/PGVector |
| 索引构建 | VectorStoreIndex | 核心检索结构                    |
| 检索     | VectorRetriever  | Top-K 搜索                      |
| 生成     | LLM（DeepSeek）  | 最终回答                        |

你需要做的只是 **拼装这几个组件**，LlamaIndex 会把检索 + Prompt 组装全部处理掉。

------

# ⭐ 二、最推荐的技术组合（给你直接答案）

### LLM：

**DeepSeek Chat API**
 （OpenAI兼容，调用简单、成本低）

### Embedding：

- **DeepSeek embedding**
- 或者更强：OpenAI text-embedding-3-large
- 或本地 BGE：bge-large-zh-v1.5（中文最强）

### 向量库：

- **Chroma（本地开发）**
- Milvus（生产）
- PGVector（企业可控、Postgres 一体化）

推荐你从 Chroma 开始，简单且够快。

------

# 🔧 三、安装依赖

```bash
pip install llama-index
pip install llama-index-llms-openai
pip install llama-index-embeddings-openai
pip install chromadb
```

（如果你想用 BGE）

```
pip install sentence-transformers
```

------

# 🔐 四、DeepSeek 配置（兼容 OpenAI API）

```python
import os

os.environ["OPENAI_API_KEY"] = "你的deepseek key"
os.environ["OPENAI_BASE_URL"] = "https://api.deepseek.com/v1"
```

**重点：DeepSeek API 是 OpenAI 格式 → LlamaIndex 直接能用。**

------

# 🧠 五、完整 RAG 构建代码（你能立即运行）

下面是一套可工作的最小代码，你可以复制直接跑。

------

## 1. 加载文档

```python
from llama_index.core import SimpleDirectoryReader

documents = SimpleDirectoryReader("docs").load_data()
```

docs 目录可以放：

- .pdf
- .md
- .txt
- 公司内部文档

------

## 2. LlamaIndex 内部组件初始化（DeepSeek + Chroma）

### LLM：DeepSeek

```python
from llama_index.llms.openai import OpenAI

llm = OpenAI(
    model="deepseek-chat",
    api_base="https://api.deepseek.com/v1",
    api_key=os.environ["OPENAI_API_KEY"],
    temperature=0.2
)
```

### Embedding：DeepSeek embedding

```python
from llama_index.embeddings.openai import OpenAIEmbedding

embed_model = OpenAIEmbedding(
    model="deepseek-embedding",
    api_base="https://api.deepseek.com/v1",
    api_key=os.environ["OPENAI_API_KEY"]
)
```

（你也可以用 bge-large 作为 embed_model —— 中文更稳）

------

## 3. 构建向量库（Chroma）

```python
import chromadb
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import StorageContext

chroma_client = chromadb.PersistentClient(path="./chroma_db")
chroma_collection = chroma_client.get_or_create_collection("rag_demo")

vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)
```

------

## 4. 构建 RAG 索引（核心）

```python
from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_documents(
    documents,
    storage_context=storage_context,
    embed_model=embed_model
)
```

------

## 5. 构建 Query Engine

```python
query_engine = index.as_query_engine(
    llm=llm,
    similarity_top_k=5,
    response_mode="compact"
)
```

------

## 6. 开始问问题

```python
response = query_engine.query("文档中提到的关键功能是什么？")
print(response)
```

至此，你已经完成了一个“能查文档并回答”的 LlamaIndex + DeepSeek RAG 应用。

------

# 🎯 六、如何提升效果（关键升级点）

## 1）设置 Metadata（提升检索准确度）

给 Node 增加 metadata：

```python
from llama_index.core.node_parser import SimpleNodeParser

parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=80)
nodes = parser.get_nodes_from_documents(documents)
```

## 2）开启上下文增强（QueryTransform）

LlamaIndex 内置：

- HyDE（生成假答案提高检索）
- Multi-Query（扩展 query）
- StepBack（先概括再检索）

例如 Multi-Query：

```python
from llama_index.query_engine.transform_query_engine import MultiStepQueryEngine

query_engine = MultiStepQueryEngine.from_defaults(base_query_engine)
```

效果显著提升。

------

# 🔥 七、DeepSeek 的最佳配置（实战经验）

### 推理模型：

- **deepseek-chat**（普通问答）
- **deepseek-reasoner**（有 chain-of-thought）

> 日常 RAG 用 chat 就够。
>  需要逻辑说明的，可以用 reasoner。

### 关键参数：

```python
llm = OpenAI(
    model="deepseek-chat",
    temperature=0.2,
    max_tokens=1024
)
```

temperature 越低答案越稳定。

------

# 🏭 八、生产级架构（你之后需要的）

推荐你最终落地为：

```
FastAPI 服务
├── /query (POST)   ← 输入问题
├── /update (POST)  ← 文档更新
├── /health
```

LlamaIndex 完全支持：

```python
from llama_index.core.api import OpenAIAgent
```

你可以接：

- 工具（数据库、Web API）
- 工作流
- 多 Agent 协作

也可以用 Dify 的 RAG 插件方式加载索引。

