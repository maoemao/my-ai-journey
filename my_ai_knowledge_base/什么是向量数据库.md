# 什么是向量数据库



------





## **一、什么是向量数据库（Vector Database）**



向量数据库是一种**专门存储和检索向量（向量化表示）的数据库**。

### **1️⃣ 背景**

- LLM 不能直接理解 PDF、文本、图片、音频等原始数据。
- 需要先将这些内容转成 **向量 embedding**（高维浮点数数组），表示语义。
- 检索时用 **相似度搜索（cosine similarity、inner product）** 找出与查询最相关的内容。



### **2️⃣ 特点**

- 支持高维向量存储（通常几十到几千维）
- 支持快速近似邻居搜索（ANN，Approximate Nearest Neighbor）
- 支持动态增删改文档
- 对比传统数据库，它专注于**语义检索，而不是精确匹配**



### **3️⃣ 常见向量数据库**

| **名称** | **特点**                        | **典型场景**       |
| -------- | ------------------------------- | ------------------ |
| FAISS    | Facebook开源，高速内存/磁盘索引 | 本地部署、原型     |
| Milvus   | 企业级，高性能                  | 海量数据、向量检索 |
| Pinecone | 云服务，简单易用                | SaaS + LLM 应用    |
| Weaviate | 开箱即用 + RAG 支持             | 知识库、问答系统   |
| Chroma   | Python 原生，轻量               | 本地/开发调试      |



------



## **二、向量数据库的作用**



在 RAG 场景中：

```
用户问题 → embedding → 向量数据库检索 → 返回相关文档 → LLM生成回答
```



- 文档先被切分成小块 → 每块生成 embedding → 存入向量数据库
- 用户提问 → 生成 query embedding → 与文档向量做相似度搜索
- 结果越接近，说明语义越相关
- LLM 利用这些检索结果生成回答，减少幻觉



------



## **三、如何把文档存入向量数据库（以 Python + LangChain 为例）**



### **1️⃣ 安装依赖**

```
pip install langchain openai faiss-cpu PyPDF2
```



------



### **2️⃣ 读取文档**

```
from langchain.document_loaders import PyPDFLoader

loader = PyPDFLoader("docs/人工智能白皮书.pdf")
docs = loader.load()  # 文档列表，每个元素包含 page_content
print(f"共加载 {len(docs)} 段文档")
```

- 你也可以用 TextLoader、UnstructuredMarkdownLoader 等





------



### **3️⃣ 文本切分**

```
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,    # 每块 800 字
    chunk_overlap=100  # 重叠 100 字，保持上下文连续
)
chunks = splitter.split_documents(docs)
print(f"切分成 {len(chunks)} 个块")
```



- 切分是为了让向量检索更精细
- chunk 越小，检索更精准，但索引越大



------



### **4️⃣ 向量化（生成 embedding）**

```
from langchain.embeddings import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings()  # 可以换 Ollama、HuggingFace Embedding
```

- 每个 chunk 都会生成一个向量（如 1536 维 float）
- embedding 向量表示该文本块的语义



------



### **5️⃣ 构建向量数据库（以 FAISS 为例）**

```
from langchain.vectorstores import FAISS

vectorstore = FAISS.from_documents(chunks, embedding_model)

# 保存到本地
vectorstore.save_local("vector_db")
```



- from_documents() 内部会把每个文本块生成 embedding 并建立索引
- save_local() 可以存档，下次直接加载，不必重新计算



------



### **6️⃣ 加载向量数据库**

```
vectorstore = FAISS.load_local("vector_db", embedding_model, allow_dangerous_deserialization=True)
```



------



### **7️⃣ 检索示例**

```
query = "人工智能白皮书中提到的核心方向是什么？"
docs = vectorstore.similarity_search(query, k=3)  # 返回最相关的 3 个文档块
for d in docs:
    print(d.page_content[:100])
```

- similarity_search() 根据 query embedding 与文档向量做相似度搜索
- 返回最语义相关的文档块，可作为 LLM 的上下文



------



## **四、总结步骤**

1. **文档读取**：PDF / Markdown / TXT → 文本
2. **文本切分**：chunk → 避免文本过长影响检索
3. **生成 embedding**：文本 → 高维向量
4. **存入向量数据库**：FAISS / Milvus / Pinecone
5. **查询时检索**：用户 query → embedding → 相似度搜索 → 返回文档
6. **结合 LLM**：把检索到的内容作为上下文 → 生成回答



------



✅ 这样做的好处：

- 文档越多越大也能快速检索
- 可以构建 RAG 问答系统或智能客服
- 大幅降低 LLM 幻觉率，提高回答准确性

------

